vocab_size: 32000
max_seq_len: 64

# Autoregressive transformer (M)
d_model: 1536
n_heads: 16
n_layers: 24
d_ff: 6144
dropout: 0.1
num_mixtures: 1024

# Flow (Jet)
jet:
  depth: 32
  block_depth: 4
  emb_dim: 512
  num_heads: 8

# Image tokenization
patch_size: 16
image_ar_dim: 128

# Optimization
learning_rate: 0.001
batch_size: 24
num_epochs: 1
torch_compile: false

# Training mode (paper path)
jetformer_training_mode: pca

patch_pca:
  pca_init_file: null
  whiten: true
  noise_std: 0.0
  add_dequant_noise: false
  input_size: [256, 256]
  patch_size: 16
  depth_to_seq: 1
  skip_pca: false

adaptor:
  kind: jet

# Loss balancing
text_loss_weight: 0.0025
image_loss_weight: 1.0

# CFG
cfg_strength: 4.0
cfg_drop_prob: 0.1

use_bfloat16_img_head: true

