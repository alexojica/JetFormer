vocab_size: 32000

# Image/tokenization
input_size: [64, 64]
patch_size: 16
# Use full token dimensionality for AR (3*ps^2 = 3*16*16 = 768)
image_ar_dim: 128            # maximize AR capacity to drive overfit and improve samples
num_mixtures: 512            # richer mixture head for memorization
use_bfloat16_img_head: true
channel_repeat: 0

# Flow (Jet) â€” more capacity than tiny to drive log|det| down
jet_depth: 32                # coupling blocks (increase from 4)
jet_block_depth: 4
jet_emb_dim: 512
jet_num_heads: 8
flow_actnorm: true
flow_invertible_dense: true
pre_factor_dim: None         # post-flow factoring (paper main)

# AR Transformer (Gemma-style)
d_model: 384
n_heads: 6
n_kv_heads: 1
n_layers: 8
d_ff: 2048
dropout: 0.0                 # remove regularization for overfit
max_seq_len: 64

# Conditioning (class-conditional ImageNet)
num_classes: 1000
class_token_length: 16
cfg_drop_prob: 0.0           # disable CFG drop for deterministic fit
cfg_strength: 4.0
cfg_mode: "reject"

# Dataset / training target
dataset: imagenet64_kaggle
kaggle_dataset_id: "ayaroshevskiy/downsampled-imagenet-64x64"

# Optimization / curriculum
batch_size: 64               # more steps/epoch -> easier overfit
learning_rate: 0.0005
weight_decay: 0.0            # remove L2 to allow memorization
opt_b1: 0.9
opt_b2: 0.999
grad_clip_norm: 10.0
num_epochs: 200
precision: bf16
torch_compile: false
grad_checkpoint_transformer: false
flow_grad_checkpoint: false

# EMA
ema:
  enabled: false

# Noise schedule / loss balance
rgb_sigma0: 0.0              # disable RGB noise during training
rgb_sigma_final: 0.0
noise_curriculum_epochs: 0   # no curriculum
latent_noise_std: 0.0        # no latent noise
text_loss_weight: 0.0
image_loss_weight: 1.0
grad_accum_steps: 1

# Epoch-level schedules
sample_every_epochs: 5
val_every_epochs: 5

# Evaluation controls
eval_no_rgb_noise: true
fid_every_epochs: 0          # disable heavy eval
is_every_epochs: 0
fid_is_num_samples: 0

# Dataloader / augmentation
num_workers: 8
max_samples: 5000            # fixed 5k subset
ignore_pad: false
random_flip_prob: 0.0        # disable augmentation

# Accelerator
accelerator: auto
device: auto

# W&B (optional)
wandb: true
wandb_offline: false
wandb_project: "jetformer-imagenet-64"
wandb_run_name: "OVERFIT-5K"
wandb_tags: ["imagenet64", "overfit", "5k"]
