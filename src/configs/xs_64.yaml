vocab_size: 32000

# Image/tokenization
input_size: [64, 64]
patch_size: 4
image_ar_dim: 32           # d = 32 kept for AR
num_mixtures: 128          # k = 128 mixtures
use_bfloat16_img_head: true

# Flow (Jet) â€” post-flow factoring, channel-wise only
jet_depth: 8               # coupling blocks
jet_block_depth: 2
jet_emb_dim: 128           # block width
jet_num_heads: 4
flow_actnorm: true
flow_invertible_dense: true
pre_factor_dim: null       # post-flow factoring (paper main)
channel_repeat: 0


# AR Transformer (Gemma-style)
d_model: 384
n_heads: 6
n_kv_heads: 1
n_layers: 8
d_ff: 1536
dropout: 0.1
max_seq_len: 64

# Dataset / training target
dataset: imagenet64_tfds

# Optimization / curriculum
batch_size: 32
learning_rate: 0.001
num_epochs: 50
precision: tf32
distributed: false
torch_compile: false

rgb_sigma0: 0.0
rgb_sigma_final: 0.0       # ImageNet: cosine to zero
latent_noise_std: 0.3
text_loss_weight: 0.0      # class-conditional: no text loss
image_loss_weight: 1.0
grad_accum_steps: 1
cfg_drop_prob: 0.0
cfg_strength: 4.0
cfg_mode: "reject"

# Epoch-level schedules
sample_every_epochs: 5
val_every_epochs: 5

# Dataloader
num_workers: 8
max_samples: null
ignore_pad: false

# Accelerator
accelerator: auto
device: auto

# W&B (optional)
wandb: true
wandb_offline: false
wandb_project: "jetformer-imagenet-64"
wandb_run_name: "XS-64"
wandb_tags: ["imagenet64", "xs"]


